## Stable Diffusion XL is Here
	- channel:: Two Minute Papers
	- {{video https://www.youtube.com/watch?v=kkYaikeLJdc}}
		- {{youtube-timestamp 68}} Gustave Doré style
		- {{youtube-timestamp 161}} Layered cake in the style of a landscape
- ## SDXL LoRA Training Guide
	- channel:: Aitrepenur
	- {{video https://youtu.be/N_zhQSx2Q3c}}
		- ### {{youtube-timestamp 337}} Dataset Prep
			- **High resolution**
			- high variation (different angles, expressions, backgrounds, clothing, etc.)
			- {{youtube-timestamp 438}} Minimum 10 images
			- {{youtube-timestamp 460}} **no image cropping**
				- #### Instance Prompt
				- {{youtube-timestamp 831}} For the **instance prompt**, don't use the "rare token" use the name of a celebrity look-a-like
					- {{youtube-timestamp 643}} use name of a celebrity that SDXL "knows" as base for the character you are trying to train
					- {{youtube-timestamp 807}} Use clipdrop to test if SDXL "knows" who the person is
				- {{youtube-timestamp 840}} **Class Prompt**
					- {{youtube-timestamp 866}} Regularization images
				- {{youtube-timestamp 1037}} Folder Structure
					- img
						- 20_sasha luss woman
							- {training images}
					- log
					- model
					- rec
		- ### {{youtube-timestamp 1078}} Model Output
			- name_of_character-instance_prompt
		- {{youtube-timestamp 1139}} Image captioning?
		- {{youtube-timestamp 1340}} Batches, epochs, and repeats
			- {{youtube-timestamp 1414}} Cycle Math
			- {{youtube-timestamp 1460}} High Batch Size
			- {{youtube-timestamp 1602}} Learning Rate
			- {{youtube-timestamp 1878}} High Network Rank
				- {{youtube-timestamp 1916}} 256 Network Rank, 1 Network Alpha
		- ### {{youtube-timestamp 2860}} Evaluating Results
	- ### Comments
		- ### [@TheKuzmann](https://www.youtube.com/channel/UCeRtMCv_IAM_uXE21S4kgng)
		  [8 hours ago (edited)](https://www.youtube.com/watch?v=N_zhQSx2Q3c&lc=UgxtXAVcS1cv_Qzr93x4AaABAg)
			- From my experience captions should be used in the following situations and in the following manner: use them in cases where you want to **generate a specific scene, subject or concept**.
			- This of course depends on the dataset you're training on - if you're training an item, you need the dataset to consist only of that item.
			- If you are training a person's face or half body and want to generate images of how that person, for example, dancing, training a model with captions that do not mention a person is dancing (or standing in a pose that implies movement, so the captions are written with the mention of it's hands in the air, describing a movement, etc.) will make it much more difficult or impossible for the model to generate a trained person dancing.
				- On the other hand, if your dataset consists of images of a person dancing, using captions will make a desired "concept" (implying a certain person dancing, i.e. standing in a pose that implies movement etc) become a variable (I've seen that for it also a term "pruned" caption is used) which is easy to call up.
				- On the other hand, in terms of style: training text encoder is undesirable, because you want to transfer a visual identity to the model, and most importantly, you want it to be "printed" on every possible prompt. In that case, only the class (style or aesthetics) is trained.
			- The most common mistake is to train a style with regularization images plus text encoder (which I did for an absurdly long time training styles in dreambooth).
				- Such a model is literally unusable and generates random images.
				- Even training textual inversion for style (which is the best style training) using captions can make it less flexible.
			- I'm writing all this from my personal experience and from all the possible tutorials that exist on internet and YouTube, and I've gone through ALL of them - including yours :-) I can't even mention how many failed models I've trained, and that's necessary to learn how to train a neural network.
		- ### [@testales](https://www.youtube.com/channel/UCVjq3P02dsQdVAEziZrIFVA)
		  [1 day ago](https://www.youtube.com/watch?v=N_zhQSx2Q3c&lc=Ugy6Zjw7qocj22DH5rF4AaABAg)
			- The deprecated section is probably labeled that way because training with **regularization images is more or less obsolete** or has very specific use cases only. The model already has learned millions of things and proably can take a few images more.
				- For concepts you may even be unable to generate regularization images in first place because the concept is not yet known.
			- By overriding training of a celebrity you are damaging the model intentionally which regularization is supposed to prevent. But because the Lora is applied only temporary this doesn't matter anyway.
		- ### [@MysteryGuitarMan](https://www.youtube.com/channel/UCyoM9uxRbTtYoORWJPYWYyg)
		  [4 hours ago (edited)](https://www.youtube.com/watch?v=N_zhQSx2Q3c&lc=UgxBUaYkJHs_X_dcSjF4AaABAg)
			- Thank you @Aitrepreneur! I love that this tutorial dispels some myths about LoRAs. Especially the random token thing... starting all the way back from "sks", now to "omhw" – when you take Lensa and other apps like that into account, think of how many millions of GPU-hours have been wasted (they could have started from "person" or "portrait"). Only one thing to mention:
			- **You don't need regularization images** unless you plan on merging in your LoRA into your checkpoint. Or some other pretty specific use cases, like de-overfitting a specific person / character / etc.
			  
			  That should speed up your training even more.