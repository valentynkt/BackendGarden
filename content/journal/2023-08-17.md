- # Attention Is All You Need
  id:: 64de453f-17bb-4332-aeea-4646096e0241
	- ![1706.03762.pdf](../assets/1706.03762_1692290758517_0.pdf)
		- ## Simplicity and Efficiency
			- The Transformer model eliminated the need for recurrent layers, which are present in RNNs and LSTMs. Instead, it relied solely on **attention mechanisms** to draw global dependencies between input and output.
				- ### Global Dependencies
					- *Global dependencies* refers to the relationships or connections between elements in a sequence that are not necessarily adjacent or close to each other.
						- In the context of natural language processing, this could mean the relationship between words or phrases that are far apart in a sentence or paragraph.
					- For example, consider the sentence:
					  id:: 64e39c28-94fb-478a-b303-dff252d5b0cb
						- *John, who was born in New York and studied in Boston for several years, now lives in San Francisco.*
						  id:: 64e39c44-1cbb-47ba-890b-7124250247b3
						- The relationship between "John" and "lives in San Francisco" is a global dependency because they are related but separated by a lot of intervening information.
					- In traditional RNNs or LSTMs, capturing such global dependencies can be challenging due to the vanishing gradient problem, where the model's ability to relate information diminishes as the distance between the relevant pieces of information increases.
						- #### Vanishing Gradient
							- In RNN and LSTM recurrent models, information is processed sequentially, and the model maintains a hidden state that tries to encapsulate the information from previous steps in the sequence.
							- This sequential nature can sometimes make it challenging for the model to maintain and utilize long-range dependencies, especially in longer sequences.
							- In other words, the AI's memory "vanishes" over distance between tokens.
				- By relying solely on attention mechanisms, the Transformer model can efficiently and effectively capture these global dependencies between input and output, **making it particularly powerful for tasks where understanding the broader context or distant relationships in the data is crucial**.