## Longnet
id:: 64a7f480-0d6a-415c-a76c-4a033e9dff58
	- tags:: [[Mindful Machines]]
	  type:: video
	  published:: false
	- ### Concept
		- remake this video
			- {{video https://www.youtube.com/watch?v=R0wBMDoFkP0}}
		- **Attention** mechanisms. As they get bigger, small step for a man, giant leap for mankind.
			- retraining models does not matter anymore
	- ### Thumbnail
		- ![thumb.jpg](../assets/thumb_1689072248498_0.jpg)
		- ![thumb1.jpg](../assets/thumb1_1689690140568_0.jpg)
		- ![thumb2.jpg](../assets/thumb2_1691000929245_0.jpg)
		- ![thumb.jpg](../assets/thumb_1691438961115_0.jpg)
		- ![thumb2.jpg](../assets/thumb2_1691438967187_0.jpg)
		- ![thumb3.jpg](../assets/thumb3_1691438977096_0.jpg)
		- ![thumb5.jpg](../assets/thumb5_1691438989356_0.jpg)
		- ![thumb7.jpg](../assets/thumb7_1691549125133_0.jpg){:height 412, :width 718}
	- ### Title
		- Superintelligence is MUCH Closer Than You Think
		- How Close is Superintelligence?
		- How ADHD for AI Yields AMAZING Results
		- Giving AI ADHD Makes It POWERFUL (This Changes Everything)
		- AI With ADHD Changes EVERYTHING
		- HUGE BREAKTHROUGHS in Generative AI
		- One Small Change, DRAMATIC AI Capability Increase
		- The AI Breakthrough NOBODY Is Talking About
		- The AI Breakthrough That Is Changing EVERYTHING
	- ### Script
		- #### Hook
			- How much do you read in a day?
				- *b-roll over book shelf*
			- Well, if you're anything like me,
				- *hand reaches and grabs book*
			- you pretend that you read a lot.
				- *Chad sitting reading book as camera zooms in*
			-
			- But however much it actually is, it pales in comparison to what LongNet can read and process.
				- *studio*
			-
			- Imagine reading the corpus of the entire internet and then synthesizing that data within your short-term memory to infer relationships and meaning within it.
				-
			-
			- That's effectively the ability Microsoft has now unlocked with LongNet. Dilated attention is an evolution of Google's 2017 self-attention mechanism.
			-
			- And it changes everything.
			-
			- LongNet can process up to 1 billion tokens of information in its context window - its "short-term memory". This is orders of magnitude more than the "large context models" out there right now.
			-
			- This is incredible progress and a huge step toward AGI and superintelligence.
			-
			- And I *really* wanted to understand how the researchers were able to do this - to get these mind-boggling results.
			-
			- So I spent the last month learning way more than I thought I ever would about matrix multiplication.
			-
			- And to my huge surprise, what I discovered changed my perspective on my entire life.
		- #### Attention Mechanisms
			- *pause for a beat.*
			-
			- LongNet has the ability to hyperfocus.
			-
			- *Drone shot of a sunrise over the city.*
			- *shot looking out at window of morning light. Chad walks up from left of frame and looks out. shot switches to chad with morning light on face sipping cup of coffee (aperture science mug).*
			-
			- It can focus on less of its input while at the same time "understanding" more of it.
				- *back to behind-Chad shot. Chad walks toward camera covering in black which wipes to black screen for graphics*
			-
			- An analogy with our own cognition is helpful here I think.
				- {{embed ((64e39c44-1cbb-47ba-890b-7124250247b3))}}
			-
			- To understand this sentence, your brain's attention mechanism is like a spotlight.
				- There are a lot of ideas in this sentence:
					- There is a person named John.
					- He was born in New York.
					- He studied in Boston for several years.
					- He now lives in San Francisco.
				- But to really understand the meaning of the sentence, you have to connect the ideas at the beginning and the end of the sentence. Only then can you realize that the idea the sentence is trying to convey is that *John lives in San Francisco*.
			-
			- The other information is helpful context but it's separating the two most important bits needed to understand the meaning.
				- *highlighting words*
			-
			- And before self-attention, that's how AI processed input too: sequentially.
				- *back to studio*
			-
			- But nowadays, your typical large language model will process this information in parallel with a multi-head attention mechanism.
				- *multi-head graphic*
			-
			- To give a high-level overview, this effectively means it will calculate vectors that relate each word to every other word in the sentence.
				- *Animation of arrows popping up over all the words.*
			-
			- In this way, it can know that *John* and *San Francisco* are highly related in this sentence.
				- The problem with this is, it's a lot of math.
				- It has to do this for every word to every other word, which creates a matrix of numbers where more math has to happen.
				- It's a lot of math.
				- GPUs can run this math in parrallel, but you can see that as the input length rises, the complexity rises at a quadratic rate.
					- *graphs*
			-
			- And this has enormous consequences. It means, to run advanced AI models, you need access to massive amounts of raw compute power.
			-
			- Given current hardware constraints and supply chain issues, this algorithmic complexity has the effect of keeping the power of intelligence accessible only to the most well-resourced organizations.
				- *b-roll showing getting rate-limited by OpenAI and billing costs of API access*
			-
			- *pause for a beat*
		- #### Hyperfocus
			- As I dove deeper and deeper into the math, I couldn't help but notice all the parallels between the dilated attention algorithm and the way my brain works.
				- *b-roll of Chad looking at computer screen, pencil and paper, counting on fingers*
			-
			- I was diagnosed with ADHD pretty recently as an adult and I've been learning a lot recently about how my brain works differently than neurotypical brains.
			-
			- The ability to hyperfocus is most often mentioned in the context of ADHD.
				- *title: Attention Deficit Hyperactivity Disorder*
			-
			- And it's often framed pretty negatively.
			-
			- For me, it usually manifests on accident as being totally and utterly absorbed in a task to the point where everything else is ignored or "tuned out" - even the passage of time.
				- *Shot of a keyboard in focus. Somebody walks in out of focus in the background. Keyboard lights up.*
			-
			- This is really awesome for learning new things very quickly.
				- *Chad's eyes darting back and forth lit up by computer screen.*
			-
			- It's not so awesome for all the things where my focus fades. Elements of my world that I don't find interesting will literally fade into the background to the point where I don't notice them at all.
				- *b-roll: dishes piling up scene*
			-
			- - that is - until I *need* to notice it.
				- *b-roll: chad runs out of cups and looks over at full sink*
			-
			- *pause a beat*
			-
			- Similarly, LongNet shims the transformer architecture with a dilated attention mechanism.
			- Instead of doing all that math to calculate vectors to every token in the sequence, as it gets further and further away from what its currently paying attention to, it skips tokens.
				- *GPT-4 conversation: write me a story about the most important things that happened today. for reference here is all the published articles of the day from some major news outlets. (news) write the story in the style of Shakespeare please.*
				- *fast text scrolling on screen to represent copy/pasting*
			-
			- One way to understand what it's doing is to think of it as speed-reading, getting a gist for where ideas are in the input sequence.
				- *GPT-4 responds: Quit paying attention to "the news". You'll feel a lot better about yourself.*
			- As it starts to understand what the input sequence is about, it can then hyperfocus its attention on the important parts it needs to answer the prompt.
				- *heat map to represent the parts it is paying attention to.*
					- it focuses in on the articles which you can make out: Bad Shit is Happening. Bad Shit Might Happen Soon in the Future. Think About the Bad Shit that Isn't Happening but *Could* Happen.
			-
			- This technique is not only more efficient, but more effective. It allows LongNet to pick up on nuanced relationships in the input data that would probably be missed by models with shorter context windows.
			-
		- #### A New Way of Thinking
			- Hyperfocus, when not used in a pejorative context, is sometimes referred to as a flow state.
				- *show book graphics on screen*
					- https://www.amazon.com/Flow-Psychology-Experience-Perennial-Classics/dp/0061339202#:~:text=Legendary%20psychologist%20Mihaly%20Csikszentmihalyi%27s%20famous,a%20total%20involvement%20with%20life.
			- A state of mind or a level of consciousness once called "the optimal state of experience" in 2008.
		-
			- In 2019, this idea surfaced again referring to it as the forgotten frontier of attention.
				- ((621b12e0-7057-4ae3-ab5d-829e713d86e1))
		-
			- This hyperfocus or flow state isn't exclusive to people with ADHD. Anybody can get there. It's just people with ADHD can get there really easily and often on accident.
			-
			- Some of the most unique minds of time have been able to call upon this ability to focus their mind on thought experiments.
				- ((87f1fcb1-2c90-4b63-a25d-9670d6288f38))
		-
			- And at the same time, they suffered from the accompanying motivation issues that arise from this way of thinking.
		-
			- When you can reach this optimal state of experience so easily on things you care about and are interested in, it makes everything else look....boring.
		- #### Trade-offs
			- And LongNet, just like people with ADHD, suffers from trade-offs, too.
			- As it speed-reads, learning just a little bit about the whole input sequence, it can potentially lose fine-grained details on the information.
				- Just like when you zoom out on a camera to capture a broader scene, you might miss some minute details if you don't focus in on them.
			- It can also lead to an AI, to find meaning and connections in everything - effectively making it susceptible to noise: irrelevant information getting in the way of understanding.
			-
			- But the analogy starts to break down there, as the way these models are trained, they've read and seen so much information, that given just a few bits of context in the input, it can predict with a surprising amount of accuracy what comes next.
			-
			- This makes LongNet extremely powerful. It has the ability to intelligently process in its short-term memory the amount of information a single human can read in a lifetime. And it can do this almost instantaneously on commodity hardware.
			-
			- And this is where my perspective on myself and my own capabilities has evolved greatly. I've grown up in a world where I've never really fit in and it took me a long time to figure out why.
			-
			- And understanding exactly how LongNet does the amazing things it does has been the key to discovering my own inner power.
			-
			- If you read through the algorithms and the math, it's definitely dense, but the mechanisms involved are beautifully simple. It's something that anybody with a little patience and coding can implement for themselves.
			-
			- In fact, a lot of people have.
				- *b-roll of OSS projects and cpp manifesto*
			-
			- Breakthroughs like this on a technical level, an individual level, and even at the societal level - this is what is driving the change in the world.
			-
			- If the Third Industrial revolution was about information and data, the Fourth is about taking that data and automating and industrializing wisdom.
			- And in that environment, as we stand here on the brink of unprecedented change, I think the most important question you should ask yourself right now is
				- whose ideas are you serving?
			-
			- Are they yours?
				- *b-roll of office workers and laughing bosses*
			-
			- Are they helping you get to that flow state - that optimal state of consciousness?
			-
			- Because with the power of AI, ideas become either the tools that will set us free or the chains that entrench the status quo forever.
			-
			- Which way we go is being decided right now. By all of us, together. Whether you are aware of it or not. The decisions we make today will have enormous impact on all of our futures.
			-
			- What will you do with *your* power?
	- ### Video Description
		- We stand at the threshold of a fourth industrial revolution, spearheaded by groundbreaking AI technologies.
		- Dive deep into the functionalities of LongNet, Microsoft’s latest venture into the frontier of artificial intelligence. With the prowess to analyze up to a billion tokens of information simultaneously in its 'short-term memory,' LongNet is setting the stage to transcend the known boundaries of machine learning and artificial intelligence.
		- Explore how LongNet's refined attention mechanism mirrors the human brain’s state of hyperfocus, previously described as the 'optimal state of consciousness.'
		- As pioneers in the Fourth Industrial Revolution where wisdom is becoming industrialized and automated, we must critically evaluate the trajectory of this potent force. Will it serve as a liberating tool or anchor us to a rigid and immutable reality? The decisions we make today have the gravity to shape our collective future.
		- Join the conversation and envision the myriad possibilities as we venture into an era of untapped potential with LongNet.
		- Music
			- Mystery Train by David O'Brien
			  License ID: m0zNEJQQqEv
			- Blue Waltz by Neil Sidwell
			  License ID: Z41zXpkoxAM
			- Curiosity Shop by David O'Brien
			  License ID: wNdPZn1X7xk
			- Synchronicity 10 (60) by Joe Henson, Alexis Smith
			  License ID: VNDJZOexDg7
			- Machines Are My Only Friends by Luke Richards
			  License ID: qEdzz3NXdYN
			- Mindset by Alex Arcoleo
			  License ID: 1aqZQd6LPLB
			- https://go.lickd.co/Music
		-
	- ### Result